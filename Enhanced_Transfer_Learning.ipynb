{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enhanced_Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO04mmr6Ub51DncFKM7qGWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btcain44/Applied_Deep_Learning/blob/main/Enhanced_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVw-joU9TXbr"
      },
      "source": [
        "## Bi-Weekly Report #5\n",
        "### Brian Cain\n",
        "#### Enhanced_Transfer_Learning.ipynb\n",
        "\n",
        "In my previous attempts I have had lackluster performance in attempting to classify the Cifar-100 dataset. The best results I got were in my Bi-Weekly Report #3 when I made a Generalist-Specialist model with 39.62% accuracy. \n",
        "\n",
        "I now will try to extend the lessons learned in the <b>Comparitive_Transfer_Learning.ipynb</b> notebook to apply transfer learning to the CIFAR-100 classification problem. Here are a couple of lessons learned about transfer learning in that notebook:\n",
        "* Add layers after ImageNet weights to give the network more ability to learn about Task B\n",
        "* Turn on Fine Tuning of Task A's weights when training Task B so the network can adjust the ImageNet weights to boost classification \n",
        "\n",
        "Now I will import the CIFAR-100 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsgKLNBJUey1",
        "outputId": "cf007bc3-99ca-4c22-aa85-d67381d45969"
      },
      "source": [
        "##Import necessary packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "##Load the CIFAR-100 dataset\n",
        "from tensorflow import keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "assert x_train.shape == (50000, 32, 32, 3)\n",
        "assert x_test.shape == (10000, 32, 32, 3)\n",
        "assert y_train.shape == (50000, 1)\n",
        "assert y_test.shape == (10000, 1)\n",
        " \n",
        "\n",
        "##Format the labels so we have shape (50000,) and (10000,) respectively \n",
        "y_train = np.array([i[0] for i in y_train])\n",
        "y_test = np.array([i[0] for i in y_test])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 6s 0us/step\n",
            "169017344/169001437 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuiAXgJpVBPs"
      },
      "source": [
        "### Transfer Learning CIFAR-100 by Going Deeper\n",
        "\n",
        "Lets craft up the enhanced architecture of the model with Fine-Tuning enabled. Here are some additional features of this architecture and what they are aimed at solving:\n",
        "* <b>Fine-Tuning ImageNet Weights:</b> The results in Comparitive_Transfer_Learning.ipynb made it obvious that transfer learning from ResNet50 with additional dense layers could only do so much. By Fine-tuning weights we can improve accuracy by learning more about the data in our specific classification problem rather than just relying on static weights designed for ImageNet\n",
        "* <b>Add Convolutions After ResNet50 ImageNet Weights:</b> Another hypothesized downfall of the transfer learning architecture used in Comparitive_Transfer_Learning.ipynb was the fact that only a single dense layer was used after the ImageNet weights to output a prediction. This time around, I made a deeper network by adding two 1x1 convolutions combined with Global Average Pooling to give the Cifar-100 network a change to learn even more about the data. However, I am concerned if over-fitting may occur because of this. \n",
        "* <b>Adam Optimizer with Learning Rate .001:</b> In previous attempts to classify the CIFAR-100 dataset, I have achieved extremely low accuracy. This time, I chose to specify a relatively low learning rate for the Adam optimizer so that the model will be less likely to miss optimal weights when Fine-Tuning the network and creating weights for new convolutions. This will add compute time but I am hoping it is worth it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii8EiOnCVGRj",
        "outputId": "1bf96d5b-6b13-46cb-f1d8-0dae484b0997"
      },
      "source": [
        "##Import the ResNet50 Architecture\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "#Obtain Imagenet model weights\n",
        "feature_extractor = ResNet50(weights='imagenet', \n",
        "                             input_shape=(32, 32, 3),\n",
        "                             include_top=False)\n",
        "\n",
        "#For better performance, make it so that we are Fine-Tuning imagenet weights during transfer learning\n",
        "feature_extractor.trainable = True\n",
        "\n",
        "#Define dimensions for the\n",
        "input_ = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "#Create layer that extracts features\n",
        "x = feature_extractor(input_, training=True)\n",
        "\n",
        "##Add additional convolutions to see if this helps network become more accuracte\n",
        "x = tf.keras.layers.Conv2D(1024, (1, 1), activation='relu', input_shape=(1,1,2048))(x)\n",
        "x = tf.keras.layers.Conv2D(612, (1, 1), activation='relu', input_shape=(1,1,1024))(x)\n",
        "\n",
        "#Perform global average pooling to condense ResNet50 output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Set the final layer with sigmoid activation function\n",
        "x = tf.keras.layers.Dense(306, activation='relu')(x)\n",
        "output_ = tf.keras.layers.Dense(100, activation='softmax')(x)\n",
        "\n",
        "#Make an instance of the transfer learning architecture\n",
        "cifar_100_deep = tf.keras.Model(input_, output_)\n",
        "\n",
        "#Compile the model\n",
        "cifar_100_deep.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "                             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                             metrics=['accuracy'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWPfAS5mckYU"
      },
      "source": [
        "Now let's split the train/validation set and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewQ4yaCaY5sE"
      },
      "source": [
        "##Conduct a step of data normalization\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "##Perform train/validation split on the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_xhje5Y4p5",
        "outputId": "dbbbeaa5-9de5-4fb3-82e1-2fde5c38c68a"
      },
      "source": [
        "##Fit and train the model\n",
        "cifar_deep_fit = cifar_100_deep.fit(x_train, y_train, batch_size=100, epochs=50, \n",
        "                                   validation_data=(x_val, y_val))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "375/375 [==============================] - 58s 155ms/step - loss: 2.8809 - accuracy: 0.2619 - val_loss: 2.7573 - val_accuracy: 0.3009\n",
            "Epoch 2/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 2.4880 - accuracy: 0.3509 - val_loss: 2.5206 - val_accuracy: 0.3564\n",
            "Epoch 3/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 2.2314 - accuracy: 0.4077 - val_loss: 2.4476 - val_accuracy: 0.3681\n",
            "Epoch 4/50\n",
            "375/375 [==============================] - 56s 151ms/step - loss: 2.0522 - accuracy: 0.4471 - val_loss: 2.2999 - val_accuracy: 0.4159\n",
            "Epoch 5/50\n",
            "375/375 [==============================] - 56s 150ms/step - loss: 1.7898 - accuracy: 0.5054 - val_loss: 2.3593 - val_accuracy: 0.4024\n",
            "Epoch 6/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 1.6539 - accuracy: 0.5390 - val_loss: 2.2957 - val_accuracy: 0.4335\n",
            "Epoch 7/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 1.4620 - accuracy: 0.5857 - val_loss: 2.2912 - val_accuracy: 0.4366\n",
            "Epoch 8/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 1.3138 - accuracy: 0.6235 - val_loss: 2.4253 - val_accuracy: 0.4266\n",
            "Epoch 9/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 1.2711 - accuracy: 0.6354 - val_loss: 2.3409 - val_accuracy: 0.4487\n",
            "Epoch 10/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 1.0923 - accuracy: 0.6794 - val_loss: 2.3688 - val_accuracy: 0.4548\n",
            "Epoch 11/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.9702 - accuracy: 0.7115 - val_loss: 2.4415 - val_accuracy: 0.4570\n",
            "Epoch 12/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.8644 - accuracy: 0.7408 - val_loss: 2.4904 - val_accuracy: 0.4608\n",
            "Epoch 13/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.8019 - accuracy: 0.7607 - val_loss: 2.5829 - val_accuracy: 0.4658\n",
            "Epoch 14/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.7212 - accuracy: 0.7849 - val_loss: 2.6478 - val_accuracy: 0.4610\n",
            "Epoch 15/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.6300 - accuracy: 0.8104 - val_loss: 2.6524 - val_accuracy: 0.4660\n",
            "Epoch 16/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.5637 - accuracy: 0.8295 - val_loss: 2.7285 - val_accuracy: 0.4621\n",
            "Epoch 17/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.5069 - accuracy: 0.8454 - val_loss: 2.7985 - val_accuracy: 0.4637\n",
            "Epoch 18/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.4760 - accuracy: 0.8568 - val_loss: 2.8461 - val_accuracy: 0.4660\n",
            "Epoch 19/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.4306 - accuracy: 0.8695 - val_loss: 2.8448 - val_accuracy: 0.4635\n",
            "Epoch 20/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.3880 - accuracy: 0.8830 - val_loss: 2.9999 - val_accuracy: 0.4641\n",
            "Epoch 21/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.3689 - accuracy: 0.8885 - val_loss: 3.0952 - val_accuracy: 0.4605\n",
            "Epoch 22/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.3584 - accuracy: 0.8926 - val_loss: 3.0862 - val_accuracy: 0.4610\n",
            "Epoch 23/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.3293 - accuracy: 0.9022 - val_loss: 3.0494 - val_accuracy: 0.4678\n",
            "Epoch 24/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.3092 - accuracy: 0.9067 - val_loss: 3.0649 - val_accuracy: 0.4670\n",
            "Epoch 25/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.2869 - accuracy: 0.9154 - val_loss: 3.0780 - val_accuracy: 0.4554\n",
            "Epoch 26/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.2824 - accuracy: 0.9156 - val_loss: 3.2360 - val_accuracy: 0.4590\n",
            "Epoch 27/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2546 - accuracy: 0.9237 - val_loss: 3.2769 - val_accuracy: 0.4620\n",
            "Epoch 28/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2450 - accuracy: 0.9277 - val_loss: 3.1471 - val_accuracy: 0.4633\n",
            "Epoch 29/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2370 - accuracy: 0.9323 - val_loss: 3.1021 - val_accuracy: 0.4606\n",
            "Epoch 30/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.2785 - accuracy: 0.9206 - val_loss: 3.2565 - val_accuracy: 0.4668\n",
            "Epoch 31/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2309 - accuracy: 0.9338 - val_loss: 3.2675 - val_accuracy: 0.4627\n",
            "Epoch 32/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.2521 - accuracy: 0.9275 - val_loss: 3.2391 - val_accuracy: 0.4634\n",
            "Epoch 33/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2257 - accuracy: 0.9360 - val_loss: 3.3769 - val_accuracy: 0.4626\n",
            "Epoch 34/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1901 - accuracy: 0.9458 - val_loss: 3.3746 - val_accuracy: 0.4722\n",
            "Epoch 35/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.2402 - accuracy: 0.9311 - val_loss: 3.2513 - val_accuracy: 0.4628\n",
            "Epoch 36/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.3724 - accuracy: 0.8986 - val_loss: 3.1719 - val_accuracy: 0.4618\n",
            "Epoch 37/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.2662 - accuracy: 0.9237 - val_loss: 3.2943 - val_accuracy: 0.4684\n",
            "Epoch 38/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1645 - accuracy: 0.9530 - val_loss: 3.4831 - val_accuracy: 0.4737\n",
            "Epoch 39/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.1949 - accuracy: 0.9445 - val_loss: 3.3864 - val_accuracy: 0.4676\n",
            "Epoch 40/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1544 - accuracy: 0.9559 - val_loss: 3.3991 - val_accuracy: 0.4674\n",
            "Epoch 41/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.1389 - accuracy: 0.9611 - val_loss: 3.4620 - val_accuracy: 0.4667\n",
            "Epoch 42/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1726 - accuracy: 0.9513 - val_loss: 3.3715 - val_accuracy: 0.4617\n",
            "Epoch 43/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.2081 - accuracy: 0.9419 - val_loss: 3.2721 - val_accuracy: 0.4609\n",
            "Epoch 44/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.1546 - accuracy: 0.9560 - val_loss: 3.5173 - val_accuracy: 0.4691\n",
            "Epoch 45/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.1492 - accuracy: 0.9582 - val_loss: 3.4194 - val_accuracy: 0.4727\n",
            "Epoch 46/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1422 - accuracy: 0.9600 - val_loss: 3.4968 - val_accuracy: 0.4708\n",
            "Epoch 47/50\n",
            "375/375 [==============================] - 57s 151ms/step - loss: 0.1624 - accuracy: 0.9562 - val_loss: 3.4066 - val_accuracy: 0.4695\n",
            "Epoch 48/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.1652 - accuracy: 0.9542 - val_loss: 3.4707 - val_accuracy: 0.4665\n",
            "Epoch 49/50\n",
            "375/375 [==============================] - 57s 152ms/step - loss: 0.2005 - accuracy: 0.9449 - val_loss: 3.4623 - val_accuracy: 0.4580\n",
            "Epoch 50/50\n",
            "375/375 [==============================] - 57s 153ms/step - loss: 0.1695 - accuracy: 0.9532 - val_loss: 3.4014 - val_accuracy: 0.4662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr_Sj6y7dPhJ"
      },
      "source": [
        "Now lets Assess the Test Results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvxM88WhekGm",
        "outputId": "e6b3fffa-c323-4c84-ba1b-02a7e936e4ac"
      },
      "source": [
        "##Evaluate the testing accuracy of the model\n",
        "print('Test Accuracy of CIFAR-100 Transfer Learned:')\n",
        "cifar_100_deep.evaluate(x_test,  y_test, verbose=0)[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of CIFAR-100 Transfer Learned:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44609999656677246"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3AtcEmfiwP5"
      },
      "source": [
        "To start off with a positive, this transfer learning technique did surpass my previous best accuray on CIFAR-100 of 39.62% with a test accuracy of 44.61%. \n",
        "\n",
        "However, there ended up being a crazy amount of over-fitting present in this CIFAR-100 classification model using ResNet50 ImageNet transfer learning. I'm not sure exactly what would account for that to happen but perhaps the addition of the 1x1 convolutional layers and a lower learning rate had something to do with it. \n",
        "\n",
        "### Transfer Learning CIFAR-100 going Less Deep and More Data\n",
        "\n",
        "One more experiment I would like to try with transfer learning to see if I can improve CIFAR-100 classification is to make the network slightly less deep and give it more training data. Here are the ways I will do so:\n",
        "* <b>Get rid of additional convolutional layers after ImageNet weights</b>\n",
        "* <b>Mixup Data Augmentation:</b> In Bi-Weekly report 2, I attempted using Mixup Data Augmentation on the CIFAR-100 dataset. Although the results weren't great, it did prevent over-fitting. I will re-train the network here using Mixup Augmentation and hope to see less over-fitting. \n",
        "\n",
        "Define function for mixup data augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZxf21qTkrCd"
      },
      "source": [
        "##Define a function that performs mixup two images \n",
        "def mixup(image1, image2, label1, label2, beta_params):\n",
        "    \n",
        "    ##Generate sample from lambda distribution \n",
        "    lambda_val = np.random.beta(beta_params[0], beta_params[1])\n",
        "    \n",
        "    ##Perform mix-up operation \n",
        "    newImg = lambda_val*image1 + (1-lambda_val)*image2\n",
        "    newLabel = round(lambda_val*label1 + (1-lambda_val)*label2)\n",
        "    \n",
        "    return newImg.astype(int), newLabel"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS-V97nFlDeg"
      },
      "source": [
        "Now perform mix-up on a segment of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D244ievxlH3x"
      },
      "source": [
        "##Alter a random 20% of the training data \n",
        "np.random.seed(0)\n",
        "rand_images = np.random.randint(1, 37500, size=int(37500*.2), dtype=int)\n",
        "\n",
        "##Create training dataset using mixup\n",
        "x_train_mixup = []\n",
        "y_train_mixup = []\n",
        "for i in rand_images:\n",
        "    mixup_result = (mixup(x_train[i],x_train[i-1],y_train[i],y_train[i-i],[.2,.2]))\n",
        "    x_train_mixup.append(mixup_result[0])\n",
        "    y_train_mixup.append(mixup_result[1])\n",
        "\n",
        "##Translate training data into numpy arrays\n",
        "x_train_mixup = np.array(x_train_mixup)\n",
        "y_train_mixup = np.array(y_train_mixup)\n",
        "\n",
        "##Concatenate new data onto existing training data\n",
        "x_train_mixup = np.concatenate((x_train, x_train_mixup), axis=0)\n",
        "y_train_mixup = np.concatenate((y_train, y_train_mixup),axis=None)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qVXQ8d-lbB9"
      },
      "source": [
        "Now we will make the alterations to the model described above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK4i9RXVlnkI"
      },
      "source": [
        "#Obtain Imagenet model weights\n",
        "feature_extractor = ResNet50(weights='imagenet', \n",
        "                             input_shape=(32, 32, 3),\n",
        "                             include_top=False)\n",
        "\n",
        "#For better performance, make it so that we are Fine-Tuning imagenet weights during transfer learning\n",
        "feature_extractor.trainable = True\n",
        "\n",
        "#Define dimensions for the\n",
        "input_ = tf.keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "#Create layer that extracts features\n",
        "x = feature_extractor(input_, training=True)\n",
        "\n",
        "#Perform global average pooling to condense ResNet50 output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Set the final layer with sigmoid activation function\n",
        "output_ = tf.keras.layers.Dense(100, activation='softmax')(x)\n",
        "\n",
        "#Make an instance of the transfer learning architecture\n",
        "cifar_100_shallow = tf.keras.Model(input_, output_)\n",
        "\n",
        "#Compile the model\n",
        "cifar_100_shallow.compile(optimizer='Adam',\n",
        "                             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                             metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZaDe2HooIlq"
      },
      "source": [
        "Also it is important to note that in this model I had to use a larger batch size of 200 over 25 epochs to speed up training because Google had previously interrupted my session for going over the GPU limits so I wanted to make sure I could fit this training session in without being interrupted.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ5ariBjlwII",
        "outputId": "d8bd542b-5f5d-4b03-e259-32cb64624401"
      },
      "source": [
        "##Fit and train the model\n",
        "cifar_shallow_fit = cifar_100_shallow.fit(x_train_mixup, y_train_mixup, batch_size=200, epochs=25, \n",
        "                                   validation_data=(x_val, y_val))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "225/225 [==============================] - 101s 218ms/step - loss: 3.8810 - accuracy: 0.1421 - val_loss: 3.9590 - val_accuracy: 0.1653\n",
            "Epoch 2/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 3.3430 - accuracy: 0.2317 - val_loss: 3.3893 - val_accuracy: 0.2366\n",
            "Epoch 3/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 2.7791 - accuracy: 0.3279 - val_loss: 3.1196 - val_accuracy: 0.3070\n",
            "Epoch 4/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 2.3928 - accuracy: 0.4073 - val_loss: 2.8685 - val_accuracy: 0.3496\n",
            "Epoch 5/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 2.2185 - accuracy: 0.4477 - val_loss: 2.8156 - val_accuracy: 0.3532\n",
            "Epoch 6/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 1.9735 - accuracy: 0.5015 - val_loss: 2.5080 - val_accuracy: 0.3919\n",
            "Epoch 7/25\n",
            "225/225 [==============================] - 48s 211ms/step - loss: 1.6819 - accuracy: 0.5713 - val_loss: 2.5389 - val_accuracy: 0.3991\n",
            "Epoch 8/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 1.4938 - accuracy: 0.6200 - val_loss: 2.5134 - val_accuracy: 0.3966\n",
            "Epoch 9/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 1.3074 - accuracy: 0.6696 - val_loss: 2.5576 - val_accuracy: 0.3950\n",
            "Epoch 10/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 1.2479 - accuracy: 0.6868 - val_loss: 2.5965 - val_accuracy: 0.3917\n",
            "Epoch 11/25\n",
            "225/225 [==============================] - 47s 210ms/step - loss: 1.0681 - accuracy: 0.7396 - val_loss: 2.6462 - val_accuracy: 0.4070\n",
            "Epoch 12/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.9792 - accuracy: 0.7658 - val_loss: 2.8219 - val_accuracy: 0.4033\n",
            "Epoch 13/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 0.9380 - accuracy: 0.7776 - val_loss: 2.7940 - val_accuracy: 0.3994\n",
            "Epoch 14/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 0.9051 - accuracy: 0.7892 - val_loss: 2.7697 - val_accuracy: 0.4092\n",
            "Epoch 15/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.8765 - accuracy: 0.7983 - val_loss: 2.8300 - val_accuracy: 0.4001\n",
            "Epoch 16/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.8520 - accuracy: 0.8053 - val_loss: 2.8402 - val_accuracy: 0.4027\n",
            "Epoch 17/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 0.8191 - accuracy: 0.8167 - val_loss: 2.8942 - val_accuracy: 0.4034\n",
            "Epoch 18/25\n",
            "225/225 [==============================] - 48s 211ms/step - loss: 0.7922 - accuracy: 0.8252 - val_loss: 2.8854 - val_accuracy: 0.4100\n",
            "Epoch 19/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.7624 - accuracy: 0.8343 - val_loss: 2.9638 - val_accuracy: 0.3990\n",
            "Epoch 20/25\n",
            "225/225 [==============================] - 48s 211ms/step - loss: 0.7778 - accuracy: 0.8291 - val_loss: 2.9328 - val_accuracy: 0.4098\n",
            "Epoch 21/25\n",
            "225/225 [==============================] - 48s 212ms/step - loss: 0.8017 - accuracy: 0.8206 - val_loss: 2.9150 - val_accuracy: 0.4125\n",
            "Epoch 22/25\n",
            "225/225 [==============================] - 48s 211ms/step - loss: 0.7845 - accuracy: 0.8258 - val_loss: 2.8987 - val_accuracy: 0.4066\n",
            "Epoch 23/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.7454 - accuracy: 0.8384 - val_loss: 2.8419 - val_accuracy: 0.4099\n",
            "Epoch 24/25\n",
            "225/225 [==============================] - 47s 210ms/step - loss: 0.7356 - accuracy: 0.8425 - val_loss: 2.9431 - val_accuracy: 0.4134\n",
            "Epoch 25/25\n",
            "225/225 [==============================] - 47s 211ms/step - loss: 0.7307 - accuracy: 0.8436 - val_loss: 2.9489 - val_accuracy: 0.4230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7AbzE1Gn0X-",
        "outputId": "a6e9c7b6-d2ec-44ef-cff0-78fbfd501422"
      },
      "source": [
        "##Evaluate the testing accuracy of the model\n",
        "print('Test Accuracy of CIFAR-100 Transfer Learned with Data Augmentation and Shallower Framework:')\n",
        "cifar_100_shallow.evaluate(x_test,  y_test, verbose=0)[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of CIFAR-100 Transfer Learned with Data Augmentation and Shallower Framework:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38199999928474426"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCs891AsTsZ"
      },
      "source": [
        "It might be unfair to directly compare this shallower transfer learning model to the one above because of increased batch size and decreased epochs, but it seems that my attempts to decrease over-fitting and increase accuracy have actually made the model worse. The test accuracy for this model is 38.2%, which is a decrease from the Generalist Specialist model made in report #3. So, it seems that by removing those convolutions I had added in the deeper transfer learning model, that we ended up losing some predictive power. I don't think that any predictive power was lost by adding the data augmentation because typically data augmentation is empirically shown to boost performance. \n",
        "\n",
        "## Final Thoughts\n",
        "\n",
        "The CIFAR-100 classification problem remains a tough one to solve, at least for me. However, using Transfer Learning with ImageNet weights from ResNet50 broke my previous record for best accuracy attained on the CIFAR-100 dataset. My previous record was obtained through a laborious process of making Generalist and Specialist models for coarse and fine labels in CIFAR-100. Transfer Learning required significantly less modeling and better results. It would be interesting in the future to try and potentially combine Generalist Specialist and Transfer learning frameworks to see if that works any better. Through the experiments in this notebook and in Comparitive_Transfer_Learning.ipynb, it is easy to see why Transfer Learning has become so relevant as it not only boosts performance but also efficiently recycles large models to create new accurate models with less work. "
      ]
    }
  ]
}